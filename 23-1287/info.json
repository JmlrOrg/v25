{
    "abstract": "Multi-objective learning (MOL) often arises in machine learning problems when there are multiple data modalities or tasks. One critical challenge in MOL is the potential conflict among different objectives during the optimization process. Recent works have developed various dynamic weighting algorithms for MOL, where the central idea is to find an update direction that avoids conflicts among objectives. Albeit its appealing intuition, empirical studies show that dynamic weighting methods may not outperform static ones. To understand this theory-practice gap, we focus on a stochastic variant of MGDA, the Multi-objective gradient with Double sampling (MoDo), and study the generalization performance and its interplay with optimization through the lens of algorithmic stability in the framework of statistical learning theory. We find that the key rationale behind MGDA\u2014updating along conflict-avoidant direction\u2014may hinder dynamic weighting algorithms from achieving the optimal $O(1/\\sqrt{n})$ population risk, where $n$ is the number of training samples. We further demonstrate the impact of dynamic weights on the three-way trade-off among optimization, generalization, and conflict avoidance unique in MOL. We showcase the generality of our theoretical framework by analyzing other algorithms under the framework. Experiments on various multi-task learning benchmarks are performed to demonstrate the practical applicability. Code is available at https://github.com/heshandevaka/Trade-Off-MOL.",
    "authors": [
        "Lisha Chen",
        "Heshan Fernando",
        "Yiming Ying",
        "Tianyi Chen"
    ],
    "emails": [
        "chenl21@rpi.edu",
        "fernah@rpi.edu",
        "yiming.ying@sydney.edu.au",
        "chentianyi19@gmail.com"
    ],
    "extra_links": [
        [
            "code",
            "https://github.com/heshandevaka/Trade-Off-MOL"
        ]
    ],
    "id": "23-1287",
    "issue": 193,
    "pages": [
        1,
        53
    ],
    "title": "Three-Way Trade-Off in Multi-Objective Learning: Optimization, Generalization and Conflict-Avoidance",
    "volume": 25,
    "year": 2024
}
{
    "abstract": "We study the optimization landscape of deep linear neural networks with square loss. It is known that, under weak assumptions, there are no spurious local minima and no local maxima. However, the existence and diversity of non-strict saddle points, which can play a role in first-order algorithms' dynamics, have only been lightly studied. We go a step further with a complete analysis of the optimization landscape at order $2$. Among all critical points, we characterize global minimizers, strict saddle points, and non-strict saddle points. We enumerate all the associated critical values. The characterization is simple, involves conditions on the ranks of partial matrix products, and sheds some light on global convergence or implicit regularization that has been proved or observed when optimizing linear neural networks. In passing, we provide an explicit parameterization of the set of all global minimizers and exhibit large sets of strict and non-strict saddle points.",
    "authors": [
        "El Mehdi Achour",
        "Fran{\\c{c}}ois Malgouyres",
        "S{{\\'e}}bastien Gerchinovitz"
    ],
    "emails": [
        "achour@mathc.rwth-aachen.de",
        "francois.malgouyres@math.univ-toulouse.fr",
        "sebastien.gerchinovitz@irt-saintexupery.fr"
    ],
    "id": "23-0493",
    "issue": 242,
    "pages": [
        1,
        76
    ],
    "title": "The Loss Landscape of Deep Linear Neural Networks: a Second-order Analysis",
    "volume": 25,
    "year": 2024
}
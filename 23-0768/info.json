{
    "abstract": "We investigate an extension of classical empirical risk minimization, where the hypothesis space consists of a random subspace within a given Hilbert space. Specifically, we examine the Nystr\u00f6m method where the subspaces are defined by a random subset of the data. This approach recovers Nystr\u00f6m approximations used in kernel methods as a specific case. Using random subspaces naturally leads to computational advantages, but a key question is whether it compromises the learning accuracy. Recently, the tradeo\ufb00s between statistics and computation have been explored for the square loss and self-concordant losses, such as the logistic loss. In this paper, we extend these analyses to general convex Lipschitz losses, which may lack smoothness, such as the hinge loss used in support vector machines. Our main results show the existence of various scenarios where computational gains can be achieved without sacrificing learning performance. When specialized to smooth loss functions, our analysis recovers most previous results. Moreover, it allows to consider classification problems and translate the surrogate risk bounds into classification error bounds. Indeed, this gives the opportunity to compare the e\ufb00ect of Nystr\u00f6m approximations when combined with di\ufb00erent loss functions such as the hinge or the square loss.",
    "authors": [
        "Andrea Della Vecchia",
        "Ernesto De Vito",
        "Jaouad Mourtada",
        "Lorenzo Rosasco"
    ],
    "emails": [
        "andrea.dellavecchia@epfl.ch",
        "ernesto.devito@unige.it",
        "jaouad.mourtada@ensae.fr",
        "lorenzo.rosasco@unige.it"
    ],
    "id": "23-0768",
    "issue": 360,
    "pages": [
        1,
        60
    ],
    "title": "The Nystr{{\\\"o}}m method for convex loss functions",
    "volume": 25,
    "year": 2024
}
{
    "abstract": "Semi-supervised learning is devoted to using unlabeled data to improve the performance of machine learning algorithms. In this paper, we study the semi-supervised generalized linear model (GLM) in the distributed setup. In the cases of single or multiple machines containing unlabeled data, we propose two distributed semi-supervised algorithms based on the distributed approximate Newton method. When the labeled local sample size is small, our algorithms still give a consistent estimation, while fully supervised methods fail to converge. Moreover, we theoretically prove that the convergence rate is greatly improved when sufficient unlabeled data exists. Therefore, the proposed method requires much fewer rounds of communications to achieve the optimal rate than its fully-supervised counterpart. In the case of the linear model, we prove the rate lower bound after one round of communication, which shows that rate improvement is essential. Finally, several simulation analyses and real data studies are provided to demonstrate the effectiveness of our method.",
    "authors": [
        "Jiyuan Tu",
        "Weidong Liu",
        "Xiaojun Mao"
    ],
    "emails": [
        "tujy.19@gmail.com",
        "weidongl@sjtu.edu.cn",
        "maoxj@sjtu.edu.cn"
    ],
    "id": "22-0670",
    "issue": 76,
    "pages": [
        1,
        41
    ],
    "title": "Distributed Estimation on Semi-Supervised Generalized Linear Model",
    "volume": 25,
    "year": 2024
}